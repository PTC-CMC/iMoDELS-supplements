{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f76c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sklearn \n",
    "import statistics\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9827100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "rcParams['mathtext.fontset'] = 'custom'\n",
    "rcParams['mathtext.it'] = 'DejaVu Sans:italic'\n",
    "rcParams['mathtext.bf'] = 'DejaVu Sans:italic:bold'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cac8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('../../data/raw-data/filtered_everything.csv', index_col=0)\n",
    "combined_test = pd.read_csv('../../data/splitted-data/everything/nbins-10/test_set.csv', index_col=0)\n",
    "m2575 = pd.read_csv('../../data/raw-data/unprocessed/mixed-25-75.csv', index_col=0)\n",
    "m2575_test = pd.read_csv('../../data/splitted-data/mixed2575/nbins-10/test_set.csv', index_col=0)\n",
    "m5050_test = pd.read_csv('../../data/splitted-data/mixed5050/nbins-10/test_set.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd99f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "oresults_path = '../predicted-results/original/nbins-10'\n",
    "mresults_path = '../predicted-results/mixed5050/nbins-10'\n",
    "eresults_path = '../predicted-results/everything/nbins-10'\n",
    "\n",
    "omodels_path = '../models/original'\n",
    "mmodels_path = '../models/mixed5050/nbins-10'\n",
    "emodels_path = '../models/everything/nbins-10'\n",
    "\n",
    "test_sets = ['5050', '2575', 'everything']\n",
    "\n",
    "mpoints = [100, 200, 300, 500, 1000, 2000, 2500, 'all']\n",
    "epoints = [100, 200, 300, 500, 1000, 2000, 2500, 4000, 6000, 'all']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75537add",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Consider 5050 models and everything models at the 2500 mark \n",
    "m5models = dict()\n",
    "emodels = dict()\n",
    "omodels = dict()\n",
    "# 5050 model\n",
    "for tset in ['5050', '2575', 'everything']:\n",
    "    m5models[tset] = dict()\n",
    "    emodels[tset] = dict()\n",
    "    omodels[tset] = dict()\n",
    "    \n",
    "    '''First load the original models'''\n",
    "    for target in ['COF', 'intercept']:\n",
    "        with open(f'{omodels_path}/{target}.pickle', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        with open(f'{omodels_path}/{target}.pickle', 'rb') as f:\n",
    "            features = pickle.load(f)\n",
    "        with open(f'{oresults_path}/{target}_on_{tset}.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        omodels[tset][target] = {\n",
    "                                 #'model': model,\n",
    "                                 'features': features,\n",
    "                                 'data': data,\n",
    "                                 'n_train': len(model.oob_prediction_),\n",
    "                                 'r_square': data[target]['r_square']}\n",
    "        \n",
    "    '''Then load the mixed5050 models'''\n",
    "    for point in mpoints:\n",
    "        for i in range(5):\n",
    "            for target in ['COF', 'intercept']:\n",
    "                with open(f'{mmodels_path}/set_{i}/{target}_{point}.pickle', 'rb') as f:\n",
    "                    model = pickle.load(f)\n",
    "                with open(f'{mmodels_path}/set_{i}/{target}_{point}.ptxt', 'rb') as f:\n",
    "                    features = pickle.load(f)\n",
    "                with open(f'{mresults_path}/set_{i}/{target}_{point}_on_{tset}.json', 'r') as f :\n",
    "                    data = json.load(f)\n",
    "                if i == 0:\n",
    "                    if not m5models[tset].get(target):\n",
    "                        m5models[tset][target] = dict()\n",
    "                    m5models[tset][target][point] = {\n",
    "                      #'model': model,\n",
    "                      'features': features,\n",
    "                      'data': [data],\n",
    "                      'n_train': len(model.oob_prediction_),\n",
    "                      'r_square': [data[target]['r_square']]}\n",
    "                else:\n",
    "                    m5models[tset][target][point]['data'].append(data)\n",
    "                    m5models[tset][target][point]['r_square'].append(data[target]['r_square'])\n",
    "                    \n",
    "    '''Finally load the combined models'''\n",
    "    for point in epoints:\n",
    "        # Lastly deal with the everything models\n",
    "        for i in range(5):\n",
    "            for target in ['COF', 'intercept']:\n",
    "                with open(f'{emodels_path}/set_{i}/{target}_{point}.pickle', 'rb') as f:\n",
    "                    model = pickle.load(f)\n",
    "                with open(f'{emodels_path}/set_{i}/{target}_{point}.ptxt', 'rb') as f:\n",
    "                    features = pickle.load(f)\n",
    "                with open(f'{eresults_path}/set_{i}/{target}_{point}_on_{tset}.json', 'r') as f :\n",
    "                    data = json.load(f)\n",
    "                if i == 0:\n",
    "                    if not emodels[tset].get(target):\n",
    "                        emodels[tset][target] = dict()\n",
    "                    emodels[tset][target][point] = {\n",
    "                      #'model': model,\n",
    "                      'features': features,\n",
    "                      'data': [data],\n",
    "                      'n_train': len(model.oob_prediction_),\n",
    "                      'r_square': [data[target]['r_square']]}\n",
    "                else:\n",
    "                    emodels[tset][target][point]['data'].append(data)\n",
    "                    emodels[tset][target][point]['r_square'].append(data[target]['r_square'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a2440",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tset in ['5050', '2575', 'everything']:\n",
    "    for point in mpoints:\n",
    "        for target in ['COF', 'intercept']:\n",
    "            m5models[tset][target][point]['ave_data'] = dict() \n",
    "            m5models[tset][target][point]['std_data'] = dict()\n",
    "            m5models[tset][target][point]['ratio-to-simulated'] = dict()\n",
    "            m5models[tset][target][point]['error'] = dict()\n",
    "            for j in m5models[tset][target][point]['data'][0][target]:\n",
    "                for i in range(5):\n",
    "                    if i==0 and j!='r_square':\n",
    "                        m5models[tset][target][point]['ave_data'][j] = [m5models[tset][target][point]['data'][i][target][j][f'predicted-{target}']]\n",
    "                    elif j!='r_square':\n",
    "                        m5models[tset][target][point]['ave_data'][j].append(m5models[tset][target][point]['data'][i][target][j][f'predicted-{target}'])\n",
    "                if j!='r_square':\n",
    "                    m5models[tset][target][point]['std_data'][j] = statistics.stdev(m5models[tset][target][point]['ave_data'][j])\n",
    "                    m5models[tset][target][point]['ave_data'][j] = statistics.mean(m5models[tset][target][point]['ave_data'][j])\n",
    "                    m5models[tset][target][point]['ratio-to-simulated'][j] = m5models[tset][target][point]['ave_data'][j]/m5models[tset][target][point]['data'][i][target][j][f'simulated-{target}']\n",
    "                    m5models[tset][target][point]['error'][j] = m5models[tset][target][point]['ave_data'][j] - m5models[tset][target][point]['data'][i][target][j][f'simulated-{target}']\n",
    "    for point in epoints:\n",
    "        for target in ['COF', 'intercept']:\n",
    "            emodels[tset][target][point]['ave_data'] = dict()\n",
    "            emodels[tset][target][point]['std_data'] = dict()\n",
    "            emodels[tset][target][point]['ratio-to-simulated'] = dict()\n",
    "            emodels[tset][target][point]['error'] = dict()\n",
    "            for j in emodels[tset][target][point]['data'][0][target]:\n",
    "                for i in range(5):\n",
    "                    if i==0 and j!='r_square':\n",
    "                        emodels[tset][target][point]['ave_data'][j] = [emodels[tset][target][point]['data'][i][target][str(j)][f'predicted-{target}']]\n",
    "                    elif j!='r_square':\n",
    "                        emodels[tset][target][point]['ave_data'][j].append(emodels[tset][target][point]['data'][i][target][str(j)][f'predicted-{target}'])\n",
    "                if j!='r_square':\n",
    "                    emodels[tset][target][point]['std_data'][j] = statistics.stdev(emodels[tset][target][point]['ave_data'][j])\n",
    "                    emodels[tset][target][point]['ave_data'][j] = statistics.mean(emodels[tset][target][point]['ave_data'][j])\n",
    "                    emodels[tset][target][point]['ratio-to-simulated'][j] = emodels[tset][target][point]['ave_data'][j]/emodels[tset][target][point]['data'][i][target][j][f'simulated-{target}']\n",
    "                    emodels[tset][target][point]['error'][j] = emodels[tset][target][point]['ave_data'][j] - emodels[tset][target][point]['data'][i][target][j][f'simulated-{target}']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51585627",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = {'mmodels': dict(),\n",
    "       'emodels': dict()}\n",
    "for tset in ['5050', '2575', 'everything']:\n",
    "    for point in mpoints:\n",
    "        doi['mmodels'][f'm50_{point}_on_{tset}'] = dict()\n",
    "        for target in ['COF', 'intercept']:\n",
    "            mmoi = m5models[tset][target][point]\n",
    "            mdoi = dict() \n",
    "            mdoi['terminal_group_1'] = list()\n",
    "            mdoi['terminal_group_2'] = list()\n",
    "            mdoi['terminal_group_3'] = list()\n",
    "            mdoi['frac-1'] = list() \n",
    "            mdoi['frac-2'] = list()\n",
    "            mdoi[f'ave-{target}'] = list()\n",
    "            mdoi[f'simulated-{target}'] = list()\n",
    "            mdoi['std'] = list()\n",
    "            for i in mmoi['ave_data']:\n",
    "                #mdoi['idx'].append(i)\n",
    "                mdoi['terminal_group_1'].append(mmoi['data'][0][target][i]['tg-1'])\n",
    "                mdoi['terminal_group_2'].append(mmoi['data'][0][target][i]['tg-2'])\n",
    "                mdoi['terminal_group_3'].append(mmoi['data'][0][target][i]['tg-3'])\n",
    "                mdoi['frac-1'].append(mmoi['data'][0][target][i]['frac-1'])\n",
    "                mdoi['frac-2'].append(mmoi['data'][0][target][i]['frac-2'])\n",
    "                mdoi[f'ave-{target}'].append(mmoi['ave_data'][i])\n",
    "                mdoi[f'simulated-{target}'].append(mmoi['data'][0][target][i][f'simulated-{target}'])\n",
    "                mdoi[f'std'].append(mmoi['std_data'][i])\n",
    "                                    \n",
    "                doi['mmodels'][f'm50_{point}_on_{tset}'][target] = pd.DataFrame.from_dict(mdoi)\n",
    "                \n",
    "    for point in epoints:\n",
    "        doi['emodels'][f'eve_{point}_on_{tset}'] = dict()            \n",
    "        for target in ['COF', 'intercept']:\n",
    "            emoi = emodels[tset][target][point]\n",
    "            edoi = dict() \n",
    "            edoi['terminal_group_1'] = list()\n",
    "            edoi['terminal_group_2'] = list()\n",
    "            edoi['terminal_group_3'] = list()\n",
    "            edoi['frac-1'] = list() \n",
    "            edoi['frac-2'] = list()\n",
    "            edoi[f'ave-{target}'] = list()\n",
    "            edoi[f'simulated-{target}'] = list()\n",
    "            edoi['std'] = list()\n",
    "            for i in emoi['ave_data']:\n",
    "                #mdoi['idx'].append(i)\n",
    "                edoi['terminal_group_1'].append(emoi['data'][0][target][i]['tg-1'])\n",
    "                edoi['terminal_group_2'].append(emoi['data'][0][target][i]['tg-2'])\n",
    "                edoi['terminal_group_3'].append(emoi['data'][0][target][i]['tg-3'])\n",
    "                edoi['frac-1'].append(emoi['data'][0][target][i]['frac-1'])\n",
    "                edoi['frac-2'].append(emoi['data'][0][target][i]['frac-2'])\n",
    "                edoi[f'ave-{target}'].append(emoi['ave_data'][i])\n",
    "                edoi[f'simulated-{target}'].append(emoi['data'][0][target][i][f'simulated-{target}'])\n",
    "                edoi[f'std'].append(emoi['std_data'][i])\n",
    "            \n",
    "                doi['emodels'][f'eve_{point}_on_{tset}'][target] = pd.DataFrame.from_dict(edoi)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c1e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m5050_test_m1000 = m5050_test.copy()\n",
    "m5050_test_o100 = m5050_test.copy()\n",
    "\n",
    "m2575_test_tmp = m2575_test.copy()\n",
    "mmodels_COF_1000_on_5050_tmp = doi['mmodels']['m50_1000_on_5050']['COF'].copy().rename(columns={'ave-COF': 'COF'})\n",
    "mmodels_COF_1000_on_2575_tmp = doi['mmodels']['m50_1000_on_2575']['COF'].copy().rename(columns={'ave-COF': 'COF'})\n",
    "mmodels_F0_1000_on_5050_tmp = doi['mmodels']['m50_1000_on_5050']['intercept'].copy().rename(columns={'ave-intercept': 'intercept'})\n",
    "mmodels_F0_1000_on_2575_tmp = doi['mmodels']['m50_1000_on_2575']['intercept'].copy().rename(columns={'ave-intercept': 'intercept'})\n",
    "omodels_COF_100_on_5050_tmp = pd.DataFrame.from_dict(omodels['5050']['COF']['data']['COF']).drop(columns=['r_square']).T.rename(columns={'predicted-COF': 'COF',\n",
    "                                                                                                                                         'tg-1': 'terminal_group_1',\n",
    "                                                                                                                                         'tg-2': 'terminal_group_2',\n",
    "                                                                                                                                         'tg-3': 'terminal_group_3'})\n",
    "omodels_F0_100_on_5050_tmp = pd.DataFrame.from_dict(omodels['5050']['intercept']['data']['intercept']).drop(columns=['r_square']).T.rename(columns={'predicted-intercept': 'intercept',\n",
    "                                                                                                                                                    'tg-1': 'terminal_group_1',\n",
    "                                                                                                                                                    'tg-2': 'terminal_group_2',\n",
    "                                                                                                                                                    'tg-3': 'terminal_group_3'})\n",
    "omodels_COF_100_on_5050_tmp['COF'] = omodels_COF_100_on_5050_tmp['COF'].astype('float64')\n",
    "omodels_F0_100_on_5050_tmp['intercept'] = omodels_F0_100_on_5050_tmp['intercept'].astype('float64')\n",
    "\n",
    "m5050_test_m1000.insert(0, 'set', 'M1000')\n",
    "m5050_test_o100.insert(0, 'set', 'O100')\n",
    "m2575_test_tmp.insert(0, 'set', 'M1000')\n",
    "mmodels_COF_1000_on_5050_tmp.insert(0, 'set', 'M1000')\n",
    "mmodels_COF_1000_on_2575_tmp.insert(0, 'set', 'M1000') \n",
    "mmodels_F0_1000_on_5050_tmp.insert(0, 'set', 'M1000') \n",
    "mmodels_F0_1000_on_2575_tmp.insert(0, 'set', 'M1000') \n",
    "omodels_COF_100_on_5050_tmp.insert(0, 'set', 'O100')\n",
    "omodels_F0_100_on_5050_tmp.insert(0, 'set', 'O100')\n",
    "\n",
    "m5050_test_m1000.insert(0, 'mode', 'Simulated')\n",
    "m5050_test_o100.insert(0, 'mode', 'Simulated')\n",
    "\n",
    "m2575_test_tmp.insert(0, 'mode', 'Simulated')\n",
    "mmodels_COF_1000_on_5050_tmp.insert(0, 'mode', 'Predicted')\n",
    "mmodels_COF_1000_on_2575_tmp.insert(0, 'mode', 'Predicted') \n",
    "mmodels_F0_1000_on_5050_tmp.insert(0, 'mode', 'Predicted') \n",
    "mmodels_F0_1000_on_2575_tmp.insert(0, 'mode', 'Predicted') \n",
    "omodels_COF_100_on_5050_tmp.insert(0, 'mode', 'Predicted')\n",
    "omodels_F0_100_on_5050_tmp.insert(0, 'mode', 'Predicted')\n",
    "\n",
    "COF_df = pd.concat([\n",
    "                    m5050_test_o100,\n",
    "                    m5050_test_m1000,\n",
    "                    mmodels_COF_1000_on_5050_tmp,\n",
    "                    omodels_COF_100_on_5050_tmp],\n",
    "                   ignore_index=True)\n",
    "F0_df = pd.concat([ \n",
    "                   m5050_test_o100,\n",
    "                   m5050_test_m1000,\n",
    "                   omodels_F0_100_on_5050_tmp,\n",
    "                   mmodels_F0_1000_on_5050_tmp],\n",
    "                   ignore_index=True)\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.violinplot(x='COF', y='set', data=COF_df, color=\"0.8\", scale='count', hue='mode',\n",
    "               split=True, palette='muted', inner=\"quart\")\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.yticks([], color='None')\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.violinplot(x='intercept', y='set', data=F0_df, color=\"0.8\", scale='count', hue='mode',\n",
    "               split=True, palette='muted', inner=\"quart\")\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.xlabel(r'$\\mathbf{F_0}$, nN')\n",
    "plt.yticks([], color='None')\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.savefig('./plots/violin/m1000o100_on_t5050.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ca981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
